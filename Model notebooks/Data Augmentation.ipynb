{"cells":[{"cell_type":"markdown","metadata":{"id":"TdK7I-Gi2KkJ"},"source":["# Data Augmentation"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hVNBgxlq2QAl","executionInfo":{"status":"ok","timestamp":1723973863888,"user_tz":-330,"elapsed":26201,"user":{"displayName":"Abisherk Sivakumar","userId":"09392094586802925853"}},"outputId":"62f84de2-f77c-4a84-c187-52fe97687ee7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"NOstcaWUPkuz"}},{"cell_type":"markdown","metadata":{"id":"wv5WtzsD2KkN"},"source":["**About the data:** <br>\n","The dataset contains 2 folders: yes and no which contains 253 Brain MRI Images. The folder yes contains 155 Brain MRI Images that are tumorous and the folder no contains 98 Brain MRI Images that are non-tumorous. You can find [here](https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection)."]},{"cell_type":"markdown","metadata":{"id":"GHni84q02KkO"},"source":["Since this is a small dataset, I used data augmentation in order to create more images."]},{"cell_type":"markdown","metadata":{"id":"2OMBYWzF2KkP"},"source":["Also, we could solve the data imbalance issue (since 61% of the data belongs to the tumorous class) using data augmentation."]},{"cell_type":"markdown","metadata":{"id":"IEFsdLK12KkQ"},"source":["## Import Necessary Modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNfLUw0F2KkR"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import cv2\n","import imutils\n","import matplotlib.pyplot as plt\n","from os import listdir\n","import time\n","import os\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yuhHjcmD2KkT"},"outputs":[],"source":["# Nicely formatted time string\n","def hms_string(sec_elapsed):\n","    h = int(sec_elapsed / (60 * 60))\n","    m = int((sec_elapsed % (60 * 60)) / 60)\n","    s = sec_elapsed % 60\n","    return f\"{h}:{m}:{round(s,1)}\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHNiYOl12KkU"},"outputs":[],"source":["def augment_data(file_dir, n_generated_samples, save_to_dir):\n","    \"\"\"\n","    Arguments:\n","        file_dir: A string representing the directory where images that we want to augment are found.\n","        n_generated_samples: A string representing the number of generated samples using the given image.\n","        save_to_dir: A string representing the directory in which the generated images will be saved.\n","    \"\"\"\n","\n","    #from keras.preprocessing.image import ImageDataGenerator\n","    #from os import listdir\n","\n","    data_gen = ImageDataGenerator(rotation_range=10,\n","                                  width_shift_range=0.1,\n","                                  height_shift_range=0.1,\n","                                  shear_range=0.1,\n","                                  brightness_range=(0.3, 1.0),\n","                                  horizontal_flip=True,\n","                                  vertical_flip=True,\n","                                  fill_mode='nearest'\n","                                 )\n","\n","\n","    for filename in listdir(file_dir):\n","        # load the image\n","        #print(os.path.join(file_dir, filename))\n","        image = cv2.imread(os.path.join(file_dir, filename))\n","        # reshape the image\n","        image = image.reshape((1,)+image.shape)\n","        # prefix of the names for the generated sampels.\n","        save_prefix = 'aug_' + filename[:-4]\n","        print(save_prefix)\n","        # generate 'n_generated_samples' sample images\n","        i=0\n","        for batch in data_gen.flow(x=image, batch_size=1, save_to_dir=save_to_dir,\n","                                           save_prefix=save_prefix, save_format='jpg'):\n","            i += 1\n","            if i > n_generated_samples:\n","                break"]},{"cell_type":"markdown","metadata":{"id":"w4RfgUVT2KkW"},"source":["Remember that 61% of the data (155 images) are tumorous. And, 39% of the data (98 images) are non-tumorous.<br>\n","So, in order to balance the data we can generate 9 new images for every image that belongs to 'no' class and 6 images for every image that belongs the 'yes' class.<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_MWoPKN2KkW","executionInfo":{"status":"ok","timestamp":1723975807677,"user_tz":-330,"elapsed":187369,"user":{"displayName":"Abisherk Sivakumar","userId":"09392094586802925853"}},"outputId":"7e323c29-727e-4925-f936-a94b18a41c64"},"outputs":[{"output_type":"stream","name":"stdout","text":["aug_Y47\n","aug_Y183\n","aug_Y102\n","aug_Y79\n","aug_Y91\n","aug_Y114\n","aug_Y113\n","aug_Y75\n","aug_Y38\n","aug_Y147\n","aug_Y21\n","aug_Y148\n","aug_Y56\n","aug_Y254\n","aug_Y163\n","aug_Y249\n","aug_Y100\n","aug_Y108\n","aug_Y22\n","aug_Y161\n","aug_Y112\n","aug_Y184\n","aug_Y253\n","aug_Y247\n","aug_Y86\n","aug_Y185\n","aug_Y117\n","aug_Y65\n","aug_Y24\n","aug_Y167\n","aug_Y165\n","aug_Y192\n","aug_Y162\n","aug_Y182\n","aug_Y187\n","aug_Y92\n","aug_Y245\n","aug_Y40\n","aug_Y81\n","aug_Y67\n","aug_Y15\n","aug_Y71\n","aug_Y41\n","aug_Y1\n","aug_Y66\n","aug_Y45\n","aug_Y8\n","aug_Y106\n","aug_Y9\n","aug_Y153\n","aug_Y154\n","aug_Y258\n","aug_Y193\n","aug_Y255\n","aug_Y51\n","aug_Y12\n","aug_Y27\n","aug_Y50\n","aug_Y46\n","aug_Y62\n","aug_Y33\n","aug_Y103\n","aug_Y186\n","aug_Y32\n","aug_Y92\n","aug_Y97\n","aug_Y61\n","aug_Y194\n","aug_Y160\n","aug_Y170\n","aug_Y52\n","aug_Y146\n","aug_Y115\n","aug_Y54\n","aug_Y17\n","aug_Y70\n","aug_Y59\n","aug_Y244\n","aug_Y11\n","aug_Y26\n","aug_Y4\n","aug_Y19\n","aug_Y10\n","aug_Y18\n","aug_Y257\n","aug_Y95\n","aug_Y49\n","aug_Y60\n","aug_Y259\n","aug_Y89\n","aug_Y44\n","aug_Y25\n","aug_Y159\n","aug_Y251\n","aug_Y73\n","aug_Y120\n","aug_Y104\n","aug_Y36\n","aug_Y109\n","aug_Y188\n","aug_Y35\n","aug_Y195\n","aug_Y39\n","aug_Y7\n","aug_Y77\n","aug_Y2\n","aug_Y74\n","aug_Y78\n","aug_Y30\n","aug_Y55\n","aug_Y20\n","aug_Y31\n","aug_Y105\n","aug_Y166\n","aug_Y37\n","aug_Y181\n","aug_Y96\n","aug_Y76\n","aug_Y69\n","aug_Y252\n","aug_Y101\n","aug_Y250\n","aug_Y155\n","aug_Y180\n","aug_Y42\n","aug_Y82\n","aug_Y248\n","aug_Y6\n","aug_Y58\n","aug_Y23\n","aug_Y13\n","aug_Y246\n","aug_Y242\n","aug_Y16\n","aug_Y107\n","aug_Y168\n","aug_Y98\n","aug_Y34\n","aug_Y111\n","aug_Y164\n","aug_Y14\n","aug_Y3\n","aug_Y90\n","aug_Y256\n","aug_Y85\n","aug_Y99\n","aug_Y28\n","aug_Y116\n","aug_Y158\n","aug_Y169\n","aug_Y29\n","aug_Y156\n","aug_Y243\n","aug_Y53\n","aug_Y157\n","aug_no 92\n","aug_45 no\n","aug_N19\n","aug_15 no\n","aug_no\n","aug_37 no\n","aug_3 no\n","aug_36 no\n","aug_1 no.\n","aug_N1\n","aug_N20\n","aug_N2\n","aug_50 no\n","aug_4 no\n","aug_29 no\n","aug_6 no\n","aug_no 5.\n","aug_no 6\n","aug_N5\n","aug_42 no\n","aug_24 no\n","aug_N11\n","aug_49 no\n","aug_no 100\n","aug_11 no\n","aug_44no\n","aug_17 no\n","aug_no 3\n","aug_No22\n","aug_38 no\n","aug_no 4\n","aug_5 no\n","aug_N16\n","aug_N17\n","aug_21 no\n","aug_32 no\n","aug_no 1\n","aug_No14\n","aug_22 no\n","aug_No13\n","aug_12 no\n","aug_33 no\n","aug_27 no\n","aug_34 no\n","aug_19 no\n","aug_30 no\n","aug_46 no\n","aug_9 no\n","aug_41 no\n","aug_N22\n","aug_No12\n","aug_no 98\n","aug_No16\n","aug_N21\n","aug_no 94\n","aug_no 91.\n","aug_no 10\n","aug_no 90\n","aug_10 no\n","aug_no 923\n","aug_No15\n","aug_N3\n","aug_no 89\n","aug_No20\n","aug_no 95\n","aug_no 97\n","aug_40 no\n","aug_18 no\n","aug_No17\n","aug_no 2\n","aug_23 no\n","aug_25 no\n","aug_28 no\n","aug_47 no\n","aug_no 96\n","aug_48 no.\n","aug_39 no\n","aug_13 no\n","aug_26 no\n","aug_N6\n","aug_2 no.\n","aug_No21\n","aug_20 no\n","aug_N15\n","aug_No11\n","aug_35 no\n","aug_8 no\n","aug_31 no\n","aug_no 9\n","aug_No19\n","aug_43 no\n","aug_no 7.\n","aug_7 no\n","aug_N26\n","aug_No18\n","aug_14 no\n","aug_no 8\n","aug_no 99\n","Elapsed time: 0:3:7.2\n"]}],"source":["start_time = time.time()\n","\n","augmented_data_path = '/content/drive/MyDrive/DSE project/Brain-Tumor-Detection/Aug_data'\n","\n","# augment data for the examples with label equal to 'yes' representing tumurous examples\n","augment_data(file_dir='/content/drive/MyDrive/DSE project/Brain-Tumor-Detection/yes', n_generated_samples=6, save_to_dir=augmented_data_path+'/yes')\n","# augment data for the examples with label equal to 'no' representing non-tumurous examples\n","augment_data(file_dir='/content/drive/MyDrive/DSE project/Brain-Tumor-Detection/no', n_generated_samples=9, save_to_dir=augmented_data_path+'/no')\n","\n","end_time = time.time()\n","execution_time = (end_time - start_time)\n","print(f\"Elapsed time: {hms_string(execution_time)}\")"]},{"cell_type":"markdown","metadata":{"id":"bvqO4zCo2KkX"},"source":["Let's see how many tumorous and non-tumorous examples after performing data augmentation:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mM_SzJhh2KkX"},"outputs":[],"source":["def data_summary(main_path):\n","\n","    yes_path = main_path+'/yes'\n","    no_path = main_path+'/no'\n","\n","    # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n","    m_pos = len(listdir(yes_path))\n","    # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n","    m_neg = len(listdir(no_path))\n","    # number of all examples\n","    m = (m_pos+m_neg)\n","\n","    pos_prec = (m_pos* 100.0)/ m\n","    neg_prec = (m_neg* 100.0)/ m\n","\n","    print(f\"Number of examples: {m}\")\n","    print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\")\n","    print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EBDSjNM12KkY","executionInfo":{"status":"ok","timestamp":1723975968853,"user_tz":-330,"elapsed":527,"user":{"displayName":"Abisherk Sivakumar","userId":"09392094586802925853"}},"outputId":"b19165e1-021a-49fb-cf66-a663df0c91c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of examples: 2065\n","Percentage of positive examples: 52.54237288135593%, number of pos examples: 1085\n","Percentage of negative examples: 47.45762711864407%, number of neg examples: 980\n"]}],"source":["data_summary(augmented_data_path)"]},{"cell_type":"markdown","metadata":{"id":"RpilxNGK2KkZ"},"source":["That's it for this notebook. Now, we can use the augmented data to train our convolutional neural network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WifNNo8G2Kka"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}